---
title: "Going Down to South Park to Make a Text Analysis using R"
author: "Patrik Drhlik"
date: "August 10, 2018"
output: 
  html_document: 
    css: main.css
    fig_caption: yes
---

```{r data_preparation, echo = FALSE, message = FALSE, warning = FALSE}
# install.packages("dplyr")
library(dplyr)
# install.packages("stringr")
library(stringr)
# devtools::install_github("pdrhlik/southparkr")
library(southparkr)
# install.packages("ggplot2")
library(ggplot2)
# install.packages("plotly")
library(plotly)
# install.packages("kableExtra")
library(kableExtra)

theme_set(theme_classic())

# Drop two columns that would be duplicated
imdb_ratings <- select(
	imdb_ratings,
	-episode_name,
	-air_date
)

episode_words <- process_episode_words(episode_lines, imdb_ratings, keep_stopwords = FALSE)
with_stopwords <- process_episode_words(episode_lines, imdb_ratings, keep_stopwords = TRUE)

by_episode <- group_by(episode_words, episode_name) %>%
	summarise(
		mean_sentiment_score = mean(sentiment_score, na.rm = TRUE),
		rating = user_rating[1],
		season_episode_number = season_episode_number[1],
		season_number = season_number[1]
	) %>%
	arrange(season_number, season_episode_number) %>%
	mutate(
		episode_number = 1:n(),
		episode_number_str = str_glue("S{stringr::str_pad(season_number, 2, 'left', pad = '0')}E{stringr::str_pad(season_episode_number, 2, 'left', pad = '0')}"),
		text_sent = str_glue("Episode name: {episode_name}
					Episode number: {episode_number_str}
					Mean sentiment score: {round(mean_sentiment_score, 2)}"),
		text_pop = str_glue("Episode name: {episode_name}
					Episode number: {episode_number_str}
					IMDB rating: {rating}")
	)
```

<!--
Possible article titles:
Going Down to South Park to Make a Text Analysis using R
R you ready for South Park?
5 Things to Learn About South Park by Analysing it
tags: South Park, learn r, R, text analysis
-->

![](boys.png)

*[South Park](https://en.wikipedia.org/wiki/South_Park) is an American TV show. It is well known for being very satirical. Pretty much every famous person has already been made fun of in the series. I literally watch it every day! I also do lots of analyses in R every day. I just thought to myself, why haven't I analysed South Park texts yet? And that's when I decided to combine two things I am passionate about. Read on to see how easy it was!*

<!--
Where and how did I get the data? (Just to introduce it, nothing technical)
Basic stats about the show - a number of episodes, words, stop words, etc.
Sentiment analysis of episodes.
Episode popularity over time.
-->

## So I have the idea, but where do I start?

First things first. I had to find a resource with all the text in a reasonable format. It took just a bit of Googling to find a South Park gold mine! I typed South Park scripts into Google and the very first link was exactly what I was looking for! [South Park archives](https://southpark.wikia.com/wiki/Portal:Scripts)–a page with community maintained scripts for all episodes! Isn't that great?

You can find a list of seasons on that page. And after clicking on a season, an episode list comes up. An episode page contains a nice table with two columns. The first column is a character name. And the second column is the actual line that character said. That's a perfect start.

There was one last thing I wanted to know about each episode. Their popularity! I'm sure that you know [IMDB–Internet Movie Database](https://www.imdb.com/). It contains ratings for all movies and Tv shows as well.

But how to put it all together? I wrote a simple R package called [southparkr](https://github.com/pdrhlik/southparkr) that anyone can use and do their own analyses!

## Data acquired. BINGO! Let's dig in.

The second step was to determine, what exactly do I want to analyse? I decided on doing two things:

1. Sentiment analysis of episodes,
2. Episode popularity based on IMDB ratings.

We'll get to that in a minute. We should first have a look at the data we acquired. Have a look at the following table. It summarises all episodes in a few numbers.

```{r summary_table, message = FALSE, warning = FALSE, echo = FALSE}
best_episode <- filter(by_episode, rating == max(rating))
worst_episode <- filter(by_episode, rating == min(rating))

basic_stats <- data_frame(
	text = c(
		"Number of seasons:",
		"Number of episodes:",
		"Number of words:",
		"No stopwords (a, the, this, ...):",
		"% used for analysis:",
		"Average IMDB rating:",
		str_glue("Best episode ({best_episode$rating}):"),
		str_glue("Worst episode ({worst_episode$rating}):")
	),
	figures = c(
		max(by_episode$season_number),
		nrow(by_episode),
		nrow(with_stopwords),
		nrow(episode_words),
		round((nrow(episode_words) / nrow(with_stopwords)) * 100, 2),
		round(mean(by_episode$rating, na.rm = TRUE), 2),
		str_glue_data(best_episode, "{episode_name} {episode_number_str}"),
		str_glue_data(worst_episode, "{episode_name} {episode_number_str}")
	)
) %>%
	mutate(
		figures = prettyNum(figures, " ")
	)

stats_table <- kable(basic_stats) %>%
	kable_styling() %>%
	column_spec(1, extra_css = "font-weight: bold;")

# Removing a table header
gsub("<thead>.*</thead>", "", stats_table)
```

You can see that the show has been on for 21 seasons already. All the characters combined have said almost **1 million words**! That is if we count all words. If we exclude stop words, we end up with about 300 thousand words. Stop words are preposition, articles or other very usual words.

All the episodes sustain an average rating of roughly **8.1** which is great! It seems that the show is popular. I always consider anything above rating 8 very watchable! You can also see the best and the worst episode. So in case you don't know the show, this is where you might start. It is almost guaranteed that you won't be disappointed.

## Let's dig deeper and get sentimental!

We'll tackle the first analysis now. The [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) of South Park episodes. It is a type of text analysis that scores words. The scores are *positive* and *negative* and can be expressed by numbers or words. We will be using the [AFINN dictionary](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) that scores words from **-5 to 5**. Where -5 is a very negative word, 0 is neutral and +5 is very positive.

For example, a -5 word is a **bastard** and a +5 word is **thrilled**!

All of this has been prepared for you behind the curtain. You will now see a few lines of code in R that show you a sentiment score of all episodes.

```{r sentiment_analysis, message = FALSE, warning = FALSE}
gg <- ggplot(by_episode, aes(x = episode_number, y = mean_sentiment_score, group = 1, text = text_sent)) +
	geom_col(color = "#592a88") +
	geom_smooth()

ggplotly(gg, tooltip = "text")
```

It created an **interactive plot**! You can hover over the bars to see some information. Each bar is an episode–you'll see an episode name, number and the sentiment score upon hovering.

It's just a few lines of code and the result is great! And above all, it is almost like **writing an English sentence**. This is how R programming looks like using the [Tidyverse](https://www.tidyverse.org/) suite of packages.

You can see that most of the episodes have the bar pointing down, below zero. That's mostly because the characters aren't afraid to use **dirty words**. And they do it quite a lot!

You might also notice a blue line in the plot. It shows a trend in the sentiment over time. I can say that there was a large **increase** in the score from the beginning. It peeked roughly around episode 80 and then started **falling** again. You can simply see that the used language changes somehow over time.

## Episodes, how popular are you?

```{r episode_popularity, message = FALSE, warning = FALSE}
gg<- ggplot(by_episode, aes(episode_number, rating, group = 1, text = text_pop)) +
	geom_point(color = "#592a88", alpha = 0.6, size = 3) +
	geom_vline(xintercept = 100, color = "red", linetype = "dashed") +
	geom_smooth()

ggplotly(gg, tooltip = "text")
```



100 episodes joke - https://en.wikipedia.org/wiki/Cancelled_(South_Park)

## Conclusion
