---
title: "Going Down to South Park, Part 3: TF-IDF Analysis with R"
author: "Patrik Drhlik"
date: "November 27, 2018"
output: 
  html_document: 
    css: main.css
    fig_caption: yes
    keep_md: yes
---

```{r knitr_opts, echo = FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	cache = TRUE,
	echo = FALSE
)
```

```{r library_load, echo = FALSE, cache = FALSE}
# Loading libraries used in the analysis
library(dplyr)
library(stringr)
# devtools::install_github("pdrhlik/southparkr")
library(southparkr)
library(ggplot2)
library(plotly)
library(kableExtra)
library(purrr)
library(glue)
library(tidytext)

# Set minimal ggplot2 theme for every plot
theme_set(theme_minimal())
```

```{r data_preparation}
# Drop two columns that would be duplicated
imdb_ratings <- select(
	imdb_ratings,
	-episode_name,
	-air_date
)
# Picked characters for our analysis
main_characters <- c("butters", "cartman", "kenny", "kyle", "randy", "stan")
character_colors <- c("#F2F32A", "#ED304C", "#F36904", "#57B749", "#51B4BE", "#4F74B1")
vertabelo_color <- "#592a88"
binary_colors <- character_colors[c(6, 2)]
# All episode words
episode_words <- process_episode_words(episode_lines, imdb_ratings, keep_stopwords = TRUE) %>%
	mutate(
		episode_number_str = str_glue("S{stringr::str_pad(season_number, 2, 'left', pad = '0')}E{stringr::str_pad(season_episode_number, 2, 'left', pad = '0')}"),
	) %>%
	filter(!(word %in% c("yeah", "uh", "huh", "hey", "ah")))

by_episode <- episode_words %>%
	group_by(episode_name) %>%
	summarize(
		total_words = n(),
		season_number = season_number[1],
		season_episode_number = season_episode_number[1],
		episode_number_str = episode_number_str[1],
		rating = user_rating[1],
		mean_sentiment_score = mean(sentiment_score, na.rm = TRUE)
	) %>%
	arrange(season_number, season_episode_number) %>%
	mutate(episode_order = 1:n())

episode_words <- left_join(
	episode_words,
	select(by_episode, season_number, season_episode_number, episode_order)
)
```

*Do you think that it is possible to determine what a text document is about? Read on to see how to use R to programmatically discover main topics of several South Park seasons and episodes!*

In the [second article of the series](https://academy.vertabelo.com/blog/south-park-text-data-analysis-with-r-2/), I showed you how to use R to analyze differences between South Park characters. I proved that Kenny is the naughtiest character, not Eric Cartman as I assumed.

In this last article of the series, I'll use a tf–idf analysis to guess episode and season topics. We'll see that it is a very powerful tool that can help us describe what's going on across different documents. We'll be using data from my [southparkr](https://github.com/pdrhlik/southparkr) package again and the wonderful [tidytext](https://github.com/juliasilge/tidytext) package written by Julia Silge and David Robinson.

## So what is tf–idf?

I'll start by introducing the method we'll be using. **tf–idf** stands for **term frequency–inverse document frequency**. [Wikipedia offers](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) a nice explanation: it is a numerical statistic that describes how a word is important in a certain document among a collection of documents.

Very common English words like *the*, *a*, *and*, etc. are probably contained in every text. If we were to simply count the number of occurrences of words, these would definitely be in the top.

Let's look at the top 10 occurring words in South Park overall to be sure.

```{r most_frequent_words}
most_freq_words <- episode_words %>%
	count(word, sort = TRUE) %>%
	mutate(n = prettyNum(n, " "))

colnames(most_freq_words) <- c("Word", "Number of occurrences")

most_freq_words %>%
	head(10) %>%
	kable() %>%
	kable_styling() %>%
	column_spec(1, extra_css = "font-weight: bold;")
```

See? We were right. No words with a real meaning here that would tell us what the show is about.

A few examples are in order. Imagine a book author that has written several books. Each of the books will contain specific words to its plot that won't be present in the others. tf–idf will identify these specific words for us.

You will see it later on in action with South Park as well. Each episode clearly contains different dialog lines with different words. tf–idf will reveal the most important words for each episode. Because I know South Park very well, I will personally decide if it did a good job or not.

## But how does it work?

tf–idf penalizes words that appear a lot and in a large number of documents. It's actually a combination of two different metrics as the name suggests. The first one is called **term frequency**:

$$tf = \frac{number\ of\ term\ occurrences\ in\ a\ document}{total\ number\ of\ words\ in\ a\ document}$$

Simply put, it's the number of the word occurrences in a document. We will use the raw count divided by the total number of words in a document. The second one is called **inverse document frequency**:

$$idf = ln(\frac{n_{documents}}{n_{documents\ containing\ term}})$$

This statistic tells us if the word is rare or very common across the documents. If we have 287 episodes of South Park and all 287 of them contain a word **the**, then **idf** will become $idf = ln(\frac{287}{287})=ln(1)=0$. By combining their product together, we get **tf–idf**:

$$tf\_idf = tf * idf$$

Because of that, **tf–idf** will always be 0 for words that appear in every document.

Let's do a concrete example with a word **alien**. South Park episodes will be our documents. We will calculate the **term frequency** for each episode in which it appears. Its **inverse document frequency** will be a single number. The following table captures all of these results for each episode where the word **alien** appears.

```{r alien_tf_idf_table}
alien <- filter(episode_words, word == "alien") %>%
	count(season_number, season_episode_number) %>%
	mutate(episode = glue("S{stringr::str_pad(season_number, 2, 'left', pad = '0')}E{stringr::str_pad(season_episode_number, 2, 'left', pad = '0')}")) %>%
	select(episode, n_alien = n) %>%
	left_join(
		by_episode %>% select(episode_number_str, total_words),
		by = c("episode" = "episode_number_str")) %>%
	mutate(
		tf = n_alien / total_words)

alien <- mutate(
	alien,
	idf = log(nrow(by_episode) / nrow(alien)),
	tf_idf = tf * idf
)

alien %>%
	kable() %>%
	kable_styling() %>%
	column_spec(1, extra_css = "font-weight: bold;")
```

We can see that the episode **S13E06** has the highest **tf–idf** for the word alien! It's called *Pinewood Derby* and Randy with his son Stan manage to make the first contact with alien life forms by accidentally discovering warp speed!

## Guessing topics of seasons 18, 19 and 20

I'd like to dive into analyzing three specific seasons. That's because seasons 18, 19 and 20 each have a plot that evolves throughout the episodes. This is something that the creators have rarely done before. Most of the episodes are stand-alone and don't require too much knowledge of the previous ones.

This is perfect for us because we should be able to use **tf–idf** to reveal what the seasons are about! You can imagine that it wouldn't be too meaningful for seasons where each episode has a different plot, right?

Let's examine the most significant words of these seasons in the following bar plot. The **tf–idf** x-axis values are multiplied by 1000 so that they are more readable.

```{r season_tf_idf}
season_words <- episode_words %>%
	count(season_number, word, sort = TRUE)

total_words <- season_words %>%
	group_by(season_number) %>%
	summarize(total = sum(n))

season_words <- left_join(season_words, total_words) %>%
	 bind_tf_idf(word, season_number, n) %>%
	 arrange(desc(tf_idf))

seasons_tf_idf <- season_words %>%
	group_by(season_number) %>%
	summarize(
		tf_idf = max(tf_idf),
		word = word[which.max(tf_idf)]
	)
```

```{r seasons_18_19_80}
our_season_words <- season_words %>%
	filter(season_number %in% c(18, 19, 20)) %>%
	group_by(season_number) %>%
	top_n(10, wt = tf_idf) %>%
	arrange(season_number, desc(tf_idf))

top_n_season_words <- our_season_words %>%
	arrange(desc(n)) %>%
	group_by(season_number) %>%
	top_n(10) %>%
	ungroup() %>%
	arrange(season_number, tf_idf) %>%
	# This is needed for proper bar ordering in facets
	# https://drsimonj.svbtle.com/ordering-categories-within-ggplot2-facets
	mutate(order = row_number())

ggplot(top_n_season_words, aes(order, tf_idf*1000, fill = factor(season_number))) +
	geom_col(show.legend = FALSE) +
	facet_wrap(~ season_number, scales = "free") +
	coord_flip() +
	labs(
		y = "tf-idf * 1000",
		x = "Top 10 most significant season topics"
	) +
	scale_x_continuous(
		breaks = top_n_season_words$order,
		labels = top_n_season_words$word
	) +
	scale_fill_manual(values = character_colors[c(2, 4, 6)]) +
	theme(panel.grid.minor = element_blank())
```

The storyline of [season 18](https://southpark.wikia.com/wiki/Season_Eighteen#Storyline) is focused on Randy Marsh having a musical career and calls himself **Lorde**. That's a number one word on our list, great job! They also make fun of **gluten**-free diets throughout the season. A big focus is also on everything revolving around **virtual** reality.

[Season 19](https://southpark.wikia.com/wiki/Season_Nineteen#Storyline) has even stronger messages. It begins with an announcement that there will be a new principle–**PC Principal**. He starts promoting a **PC** (politically correct) culture because South Park is obviously full of racists and intolerant people. Mr. Garrison doesn't take it anymore and starts running for a president with his running mate **Caitlyn Jenner**, a former decathlon Olympic champion, Bruce Jenner who had a sex change operation. The last important topic of the series are **ads** that negatively influence everything and even start being indistinguishable from regular people.

In [season 20](https://southpark.wikia.com/wiki/Season_Twenty#Storyline), it was all about **SkankHunt42**, Kyle's dad. He became a **troll** that enjoyed harassing all sort of people online. It ended up with a **Danish** Olympic champion (fictional) Freja Ollegard by killing herself. **Denmark** then created a worldwide service called **trolltrace.com** that was able to identify any **troll** online. Eric Cartman also started dating with Heidi Turner and tried to get to **Mars** with the help of Elon Musk's SpaceX company.

If you compare the bold words with the ones in the bar plot; you'll see that **tf–idf** did a great job with the topic identification! There were also a few words that were picked up but I haven't mentioned them. Those were mostly words that were very significant in a single episode and made it to the top 10 anyway. For example **drones** or **handicar**. Each of these words appeared in a single episode very heavily. But overall, the method did a great job!

For the sake of completeness, take a look at the next bar plot that will show you the most significant topic for each season. Just keep in mind that it doesn't have to be true for the other seasons that we haven't analyzed that closely.

```{r season_topics_plot}
ggplot(seasons_tf_idf, aes(season_number, tf_idf, label = word)) +
	geom_col(fill = vertabelo_color) +
	geom_text(aes(y = 0), col = "white", hjust = -.05) +
	scale_x_continuous(breaks = 1:21) +
	labs(
		x = "Season number",
		y = "tf-idf"
	) +
	coord_flip() +
	theme(panel.grid.minor = element_blank())
```

## What are the 3 most popular episodes about?

I'll do the same plot I did for the three seasons but for the 3 most popular episodes according to IMDB. Because episodes generally have a single main theme, the results should be more accurate. Well, let's find out!

```{r episode_tf_idf}
ep_words <- episode_words %>%
	count(episode_order, word, sort = TRUE)

ep_total_words <- ep_words %>%
	group_by(episode_order) %>%
	summarize(total = sum(n))

ep_words <- left_join(ep_words, ep_total_words) %>%
	 bind_tf_idf(word, episode_order, n) %>%
	 arrange(desc(tf_idf))

eps_tf_idf <- ep_words %>%
	group_by(episode_order) %>%
	summarize(
		tf_idf = max(tf_idf),
		word = word[which.max(tf_idf)]
	)

top_episodes <- ep_words %>%
	group_by(episode_order) %>%
	top_n(10, wt = tf_idf) %>%
	arrange(episode_order, desc(tf_idf)) %>%
	filter(episode_order %in% c(69, 147, 92))

top_n_episode_words <- top_episodes %>%
	arrange(desc(n)) %>%
	group_by(episode_order) %>%
	top_n(10) %>%
	ungroup() %>%
	arrange(episode_order, tf_idf) %>%
	# This is needed for proper bar ordering in facets
	# https://drsimonj.svbtle.com/ordering-categories-within-ggplot2-facets
	mutate(order = row_number())

ggplot(top_n_episode_words, aes(order, tf_idf*100, fill = factor(episode_order, labels = c("Scott Tenorman Must Die", "The Return of the Fellowship of the Ring to the Two Towers", "Make Love, Not Warcraft")))) +
	geom_col(show.legend = FALSE) +
	facet_wrap(~ factor(episode_order, labels = c("Scott Tenorman Must Die", "The Return of the\n Fellowship of the Ring \nto the Two Towers", "Make Love, Not Warcraft")), scales = "free") +
	coord_flip() +
	labs(
		y = "tf-idf * 100",
		x = "Top 10 most significant episode topics"
	) +
	scale_x_continuous(
		breaks = top_n_episode_words$order,
		labels = top_n_episode_words$word
	) +
	scale_fill_manual(values = character_colors[c(2, 4, 6)]) +
	theme(panel.grid.minor = element_blank())
```

I'll try to describe each of the three episodes using a few sentences with its true storyline. **Beware! Spoilers ahead!**

**Scott Tenorman Must Die**: **Scott Tenorman**, a 9th-grader, sells his **pubes** to Eric Cartman. Eric realises later on that he got tricked and wants to get back at Scott. He trains Mr. **Denskins's** **pony** to **bite** off Scott's wiener. Eric ends up having Scott's parents killed and serves them to Scott in a **chilli** con carne on a chilli festival. The episode ends with Scott's favorite band, **Radiohead** coming to the festival and mocking Scott.

**The Return of the Fellowship of the Ring to the Two Towers**: Stan's parents rent a **porno videotape** called **Backdoor** sl\*\*s 9 and **The Lord of the Rings**. The boys are given a **quest** to deliver the LOTR movie to Butters. The two tapes got mixed though and Butters is given the **porno** tape instead.

**Make Love, Not Warcraft**: The boys play World of **Warcraft** but encounter a **player** that is even stronger than the **admins** who starts killing innocent **players**. Their only way to level up to fight the bully **character** is to start killing **computer** generated **boars**. Once they evolve their **characters** enough, their only chance is to use the **sword of the thousand truths** to win the fight!

If you know these episodes, you must agree with these amazing results! Let's end this section with an overview of main topics for each episode. You can explore the results on the following interactive plot. I put the IMDB ratings on the y-axis so that you can only explore the popular episodes if you wanted.

```{r}
by_episode <- inner_join(by_episode, eps_tf_idf) %>%
	mutate(text_hover = str_glue("Episode name: {episode_name}
					Episode number: {episode_number_str}
					IMDB rating: {rating}
					Characteristic word: {word}"))
g <- ggplot(by_episode, aes(episode_order, rating)) +
	geom_point(aes(text = text_hover), alpha = 0.6, size = 3, col = vertabelo_color) +
	labs(
		x = "Episode number",
		y = "IMDB rating"
	)

ggplotly(g, tooltip = "text")
```

## End of the series

This was the last article from the South Park text mining series. You learned how to describe document topics using **tf–idf** analysis. It is a more sophisticated method than just using a raw word count of words that are not stop-words. Mainly because it can penalize words that are common across a set of documents.

All the code that produced this document including the plots can be found on my [Github page](https://github.com/pdrhlik/vertabelo/tree/master/south-park-tf-idf) as usual. You can try to analyze episodes or seasons that you like using the same code.

If you need a bit more practice in R to do that, check out the [Data Visualization 101](https://academy.vertabelo.com/course/data-visualization-101) course on [Vertabelo academy](https://academy.vertabelo.com/). They will show you how to use the **ggplot2** package that I use to produce my plots.

Reach out to me if you would like any help with some of your analyses! Patrik out.
