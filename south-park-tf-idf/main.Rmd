---
title: "Going Down to South Park, Part 3: TF-IDF Analysis with R"
author: "Patrik Drhlik"
date: "November 27, 2018"
output: 
  html_document: 
    css: main.css
    fig_caption: yes
    keep_md: yes
---

```{r knitr_opts, echo = FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	cache = TRUE,
	echo = FALSE
)
```

```{r library_load, echo = FALSE, cache = FALSE}
# Loading libraries used in the analysis
library(dplyr)
library(stringr)
# devtools::install_github("pdrhlik/southparkr")
library(southparkr)
library(ggplot2)
library(plotly)
library(kableExtra)
library(purrr)
library(glue)
library(tidytext)

# Set minimal ggplot2 theme for every plot
theme_set(theme_minimal())
```

```{r data_preparation}
# Drop two columns that would be duplicated
imdb_ratings <- select(
	imdb_ratings,
	-episode_name,
	-air_date
)
# Picked characters for our analysis
main_characters <- c("butters", "cartman", "kenny", "kyle", "randy", "stan")
character_colors <- c("#F2F32A", "#ED304C", "#F36904", "#57B749", "#51B4BE", "#4F74B1")
vertabelo_color <- "#592a88"
binary_colors <- character_colors[c(6, 2)]
# All episode words
episode_words <- process_episode_words(episode_lines, imdb_ratings, keep_stopwords = TRUE) %>%
	mutate(
		episode_number_str = str_glue("S{stringr::str_pad(season_number, 2, 'left', pad = '0')}E{stringr::str_pad(season_episode_number, 2, 'left', pad = '0')}"),
	) %>%
	filter(!(word %in% c("yeah", "uh", "huh", "hey", "ah")))

by_episode <- episode_words %>%
	group_by(episode_name) %>%
	summarize(
		total_words = n(),
		season_number = season_number[1],
		season_episode_number = season_episode_number[1],
		episode_number_str = episode_number_str[1],
		rating = user_rating[1],
		mean_sentiment_score = mean(sentiment_score, na.rm = TRUE)
	) %>%
	arrange(season_number, season_episode_number) %>%
	mutate(episode_order = 1:n())

episode_words <- left_join(
	episode_words,
	select(by_episode, season_number, season_episode_number, episode_order)
)
```

*Do you think that it is possible to determine what a text document is about? Read on to see how to use R to programatically discover main topics of several South Park seasons and episodes!*

In the [second article of the series](https://academy.vertabelo.com/blog/south-park-text-data-analysis-with-r-2/), I showed you how to use R to analyze differences between South Park characters. I proved that Kenny is the naughtiest character, not Eric Cartman as I assumed.

In this last article of the series, I'll use a tf–idf analysis to guess episode and season topics. We'll see that it is a very powerful tool that can help us describe what's going on across different documents. We'll be using data from my [southparkr](https://github.com/pdrhlik/southparkr) package again and the wonderful [tidytext](https://github.com/juliasilge/tidytext) package written by Julia Silge and David Robinson.

## So what is tf–idf?

I'll start by introducing the method we'll be using. **tf–idf** stands for **term frequency–inverse document frequency**. [Wikipideia offers](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) a nice explanation: it is a numerical statistic that describes how a word is important in a certain document among a collection of documents.

Very common English words like *the*, *a*, *and*, etc. are probably contained in every text. If we were to simply count the number of occurences of words, these would definitely be in the top.

Let's look at the top 10 occuring words in South Park overall to be sure.

```{r most_frequent_words}
most_freq_words <- episode_words %>%
	count(word, sort = TRUE) %>%
	mutate(n = prettyNum(n, " "))

colnames(most_freq_words) <- c("Word", "Number of occurences")

most_freq_words %>%
	head(10) %>%
	kable() %>%
	kable_styling() %>%
	column_spec(1, extra_css = "font-weight: bold;")
```

See? We were right. No words with a real meaning here that would tell us what the show is about.

A few examples are in order. Imagine a book author that has written several books. Each of the books will contain specific words to its plot that won't be present in the others. tf–idf will identify these specific words for us.

You will see it later on in action with South Park as well. Each episode clearly contains different dialog lines with different words. tf–idf will reveal the most important words for each episode. Because I know South Park very well, I will personally decide if it did a good job or not.

## But how does it work?

tf–idf penalizes words that appear a lot and in a large number of documents. It's actually a combination of two different metrics as the name suggests. The first one is called **term frequency**:

$$tf = \frac{number\ of\ term\ occurences\ in\ a\ document}{total\ number\ of\ words\ in\ a\ document}$$

Simply put, it's the number of the word occurences in a document. We will use the raw count divided by the total number of words in a document. The second one is called **inverse document frequency**:

$$idf = ln(\frac{n_{documents}}{n_{documents\ containing\ term}})$$

This statistic tells us if the word is rare or very common across the documents. If we have 287 episodes of South Park and all 287 of them contain a word **the**, then **idf** will become $idf = ln(\frac{287}{287})=ln(1)=0$. By combining their product together, we get **tf–idf**:

$$tf\_idf = tf * idf$$

Because of that, **tf–idf** will always be 0 for words that appear in every document.

Let's do a concrete example with a word **alien**. South Park episodes will be our documents. We will calculate the **term frequency** for each episode in which it appears. Its **inverse document frequency** will be a single number. The following table captures all of these results for each episode where the word **alien** appears.

```{r alien_tf_idf_table}
alien <- filter(episode_words, word == "alien") %>%
	count(season_number, season_episode_number) %>%
	mutate(episode = glue("S{stringr::str_pad(season_number, 2, 'left', pad = '0')}E{stringr::str_pad(season_episode_number, 2, 'left', pad = '0')}")) %>%
	select(episode, n_alien = n) %>%
	left_join(
		by_episode %>% select(episode_number_str, total_words),
		by = c("episode" = "episode_number_str")) %>%
	mutate(
		tf = n_alien / total_words)

alien <- mutate(
	alien,
	idf = log(nrow(by_episode) / nrow(alien)),
	tf_idf = tf * idf
)

alien %>%
	kable() %>%
	kable_styling() %>%
	column_spec(1, extra_css = "font-weight: bold;")
```

We can see that the episode **S13E06** has the highest **tf–idf** for the word alien! It's called *Pinewood Derby* and Randy with his son Stan manage to make the first contact with alien life forms by accidentally discovering warp speed!

## Guessing topics of seasons 18, 19 and 20

I'd like to dive into analyzing three specific seasons. That's because seasons 18, 19 and 20 each have a plot that evolves throughout the episodes. This is something that the creators have rarely done before. Most of the episodes are stand-alone and don't require too much knowledge of the previous ones.

This is perfect for us because we should be able to use **tf–idf** to reveal what the seasons are about! You can imagine that it wouldn't be to meaningful for seasons where each episode has a different plot, right?

```{r season_tf_idf}
season_words <- episode_words %>%
	count(season_number, word, sort = TRUE)

total_words <- season_words %>%
	group_by(season_number) %>%
	summarize(total = sum(n))

season_words <- left_join(season_words, total_words) %>%
	 bind_tf_idf(word, season_number, n) %>%
	 arrange(desc(tf_idf))

seasons_tf_idf <- season_words %>%
	group_by(season_number) %>%
	summarize(
		tf_idf = max(tf_idf),
		word = word[which.max(tf_idf)]
	)
```

```{r}
our_season_words <- season_words %>%
	filter(season_number %in% c(18, 19, 20)) %>%
	group_by(season_number) %>%
	top_n(10, wt = tf_idf) %>%
	arrange(season_number, desc(tf_idf))
```

```{r}
top_n_season_words <- our_season_words %>%
	arrange(desc(n)) %>%
	group_by(season_number) %>%
	top_n(10) %>%
	ungroup() %>%
	arrange(season_number, tf_idf) %>%
	# This is needed for proper bar ordering in facets
	# https://drsimonj.svbtle.com/ordering-categories-within-ggplot2-facets
	mutate(order = row_number())

ggplot(top_n_season_words, aes(order, tf_idf, fill = factor(season_number))) +
	geom_col(show.legend = FALSE) +
	facet_wrap(~ season_number, scales = "free") +
	coord_flip() +
	labs(
		y = "tf–idf",
		x = "Top 10 most significant season topics"
	) +
	scale_x_continuous(
		breaks = top_n_season_words$order,
		labels = top_n_season_words$word
	) +
	scale_fill_manual(values = character_colors[c(2, 4, 6)])
```


```{r episode_tf_idf}
ep_words <- episode_words %>%
	count(episode_order, word, sort = TRUE)

ep_total_words <- ep_words %>%
	group_by(episode_order) %>%
	summarize(total = sum(n))

ep_words <- left_join(ep_words, ep_total_words) %>%
	 bind_tf_idf(word, episode_order, n) %>%
	 arrange(desc(tf_idf))

eps_tf_idf <- ep_words %>%
	group_by(episode_order) %>%
	summarize(
		tf_idf = max(tf_idf),
		word = word[which.max(tf_idf)]
	)

a <- ep_words %>%
	group_by(episode_order) %>%
	top_n(5, wt = tf_idf) %>%
	arrange(episode_order, desc(tf_idf))
```

```{r}
by_episode <- inner_join(by_episode, eps_tf_idf) %>%
	mutate(text_hover = str_glue("Episode name: {episode_name}
					Episode number: {episode_number_str}
					IMDB rating: {rating}
					Characteristic word: {word}"))
g <- ggplot(by_episode, aes(episode_order, rating)) +
	geom_point(aes(text = text_hover), alpha = 0.6, size = 3, col = vertabelo_color)

ggplotly(g, tooltip = "text")
```

```{r season_topics_plot}
ggplot(seasons_tf_idf, aes(season_number, tf_idf, label = word)) +
	geom_col(fill = vertabelo_color) +
	geom_text(aes(y = 0), col = "white", hjust = -.05) +
	scale_x_continuous(breaks = 1:21) +
	coord_flip() +
	theme(panel.grid.minor = element_blank())
```

```{r}
top_seasons <- by_episode %>%
	group_by(season_number) %>%
	summarize(mean_rating = mean(rating)) %>%
	arrange(desc(mean_rating))
```

## Characterising topics of top 5 popular episodes

## End of the series
