---
title: "Going Down to South Park to Make a Text Analysis using R"
author: "Patrik Drhlik"
date: "August 14, 2018"
output: 
  html_document: 
    css: main.css
    fig_caption: yes
    keep_md: yes
---

```{r data_preparation, echo = FALSE, message = FALSE, warning = FALSE}
# install.packages("dplyr")
library(dplyr)
# install.packages("stringr")
library(stringr)
# devtools::install_github("pdrhlik/southparkr")
library(southparkr)
# install.packages("ggplot2")
library(ggplot2)
# install.packages("plotly")
library(plotly)
# install.packages("kableExtra")
library(kableExtra)

theme_set(theme_classic())

# Drop two columns that would be duplicated
imdb_ratings <- select(
	imdb_ratings,
	-episode_name,
	-air_date
)

episode_words <- process_episode_words(episode_lines, imdb_ratings, keep_stopwords = FALSE)
with_stopwords <- process_episode_words(episode_lines, imdb_ratings, keep_stopwords = TRUE)

by_episode <- group_by(episode_words, episode_name) %>%
	summarise(
		mean_sentiment_score = mean(sentiment_score, na.rm = TRUE),
		rating = user_rating[1],
		season_episode_number = season_episode_number[1],
		season_number = season_number[1]
	) %>%
	arrange(season_number, season_episode_number) %>%
	mutate(
		episode_number = 1:n(),
		episode_number_str = str_glue("S{stringr::str_pad(season_number, 2, 'left', pad = '0')}E{stringr::str_pad(season_episode_number, 2, 'left', pad = '0')}"),
		text_sent = str_glue("Episode name: {episode_name}
					Episode number: {episode_number_str}
					Mean sentiment score: {round(mean_sentiment_score, 2)}"),
		text_pop = str_glue("Episode name: {episode_name}
					Episode number: {episode_number_str}
					IMDB rating: {rating}")
	)
```

![](boys.png)

*Have you ever liked a TV show so much that watching it wasn’t enough anymore? That you wanted to know things nobody would tell you? Read on and see how I used R to see what’s inside South Park!*

## So I have the idea, but where do I start?

[South Park](https://en.wikipedia.org/wiki/South_Park) is an American TV show for adults. It is well known for being very satirical. Pretty much every famous person has already been made fun of in the series. I literally watch it every day! I also do lots of analyses in R every day. I just thought to myself, why haven't I analysed South Park texts yet? What is the overall sentiment of the show? How does the episode popularity evolve over time? Who is the naughtiest character? Or are more naughty episodes also more popular? I’ll answer these and more question in this article series about South Park.

But first things first. I had to find a resource with all the text in a reasonable format. It took just a bit of Googling to find a South Park gold mine! I typed South Park scripts into Google and the very first link was exactly what I was looking for! [South Park archives](https://southpark.wikia.com/wiki/Portal:Scripts)–a page with community maintained scripts for all episodes! Isn't that great?

You can find a list of seasons on that page. And after clicking on a season, an episode list comes up. An episode page contains a nice table with two columns. The first column is a character name. And the second column is the actual line that character said. That's a perfect start.

There was one last thing I wanted to know about each episode. Their popularity! I'm sure that you know [IMDB–Internet Movie Database](https://www.imdb.com/). It contains ratings for all movies and Tv shows as well.

But how to put it all together? I wrote an R package called [southparkr](https://github.com/pdrhlik/southparkr) that anyone can use and do their own analyses! That package downloads all the information described above and makes it conveniently available. It simply does the hard work for you and allows you to focus on your analyses.

## Data acquired. BINGO! Let's dig in.

The second step was to determine, what exactly do I want to analyse? In this article, I decided on doing two things:

1. Sentiment analysis of episodes,
2. Episode popularity based on IMDB ratings.

We'll get to that in a minute. We should first have a look at the data we acquired. Have a look at the following table. It summarises all episodes in a few numbers.

```{r summary_table, message = FALSE, warning = FALSE, echo = FALSE}
best_episode <- filter(by_episode, rating == max(rating))
worst_episode <- filter(by_episode, rating == min(rating))

basic_stats <- data_frame(
	text = c(
		"Number of seasons:",
		"Number of episodes:",
		"Number of words:",
		"No stopwords (a, the, this, ...):",
		"% used for analysis:",
		"Average IMDB rating:",
		str_glue("Best episode ({best_episode$rating}):"),
		str_glue("Worst episode ({worst_episode$rating}):")
	),
	figures = c(
		max(by_episode$season_number),
		nrow(by_episode),
		nrow(with_stopwords),
		nrow(episode_words),
		round((nrow(episode_words) / nrow(with_stopwords)) * 100, 2),
		round(mean(by_episode$rating, na.rm = TRUE), 2),
		str_glue_data(best_episode, "{episode_name} {episode_number_str}"),
		str_glue_data(worst_episode, "{episode_name} {episode_number_str}")
	)
) %>%
	mutate(
		figures = prettyNum(figures, " ")
	)

stats_table <- kable(basic_stats) %>%
	kable_styling() %>%
	column_spec(1, extra_css = "font-weight: bold;")

# Removing a table header
gsub("<thead>.*</thead>", "", stats_table)
```

You can see that the show has been on for 21 seasons already. All the characters combined have said almost **1 million words**! That is if we count all words. If we exclude stop words, we end up with about 300 thousand words. Stop words are preposition, articles or other very usual words.

All the episodes sustain an average rating of roughly **8.1** which is great! It seems that the show is popular. I always consider anything above rating 8 very watchable! You can also see the best and the worst episode. So in case you don't know the show, this is where you might start. It is almost guaranteed that you won't be disappointed.

## Let's get sentimental, let's get dirty!

We'll tackle the first analysis now. The sentiment analysis of South Park episodes is a type of text analysis that scores words. The scores are *positive* and *negative* and can be expressed by numbers or words. We will be using the [AFINN dictionary](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) that scores words from **-5 to 5**. Where -5 is a very negative word, 0 is neutral and +5 is very positive.

For example, a -5 word is a **bastard** and a +5 word is **thrilled**!

All of this has been prepared for you behind the curtain. You will now see a few lines of code in R that show you a sentiment score of all episodes.

```{r sentiment_analysis, message = FALSE, warning = FALSE}
gg <- ggplot(by_episode, aes(x = episode_number, y = mean_sentiment_score, group = 1, text = text_sent)) +
	geom_col(color = "#592a88") +
	geom_smooth()

ggplotly(gg, tooltip = "text")
```

It created an **interactive plot**! You can hover over the bars to see some information. Each bar is an episode–you'll see an episode name, number and the sentiment score upon hovering.

It's just a few lines of code and the result is great! And above all, it is almost like **writing an English sentence**. This is how R programming looks like using the [Tidyverse](https://www.tidyverse.org/) suite of packages.

**[“Intro to Tidyverse” course banner]**

You can see that most of the episodes have the bar pointing down, below zero. That's mostly because the characters aren't afraid to use **dirty words**. And they do it quite a lot!

You might also notice a blue line in the plot. It shows a trend in the sentiment over time. I can say that there was a large **increase** in the score from the beginning. It peaked roughly around episode 80 and then started **falling** again. You can simply see that the used language changes somehow over time.

## Episodes, how popular are you?

That was pretty cool, huh? We can do something very similar with the **episode popularity**. I'll show you a different kind of plot here. Because the ratings can't fall below zero, it is better to use points instead of bars.

The data have been prepared again. The following code produces an interactive plot of South Park episode ratings.

```{r episode_popularity, message = FALSE, warning = FALSE}
gg<- ggplot(by_episode, aes(episode_number, rating, group = 1, text = text_pop)) +
	geom_point(color = "#592a88", alpha = 0.6, size = 3) +
	geom_vline(xintercept = 100, color = "red", linetype = "dashed") +
	geom_smooth()

ggplotly(gg, tooltip = "text")
```

Each point represents an episode. If you hover above it, you can see the episode name along with its rating. Can you find the best and the worst episode we talked about earlier? **Give it a try!**

I have also included a trend line. It helps us determine how the popularity develops over time. **Do you see any pattern here?** Take a look at the trend line after the vertical red line. Up to that point, the popularity increases. After that, it keeps falling down.

Funny thing is that the creators themselves made a joke that a **TV show shouldn't go past 100 episodes**. It's an episode called [Cancelled](https://en.wikipedia.org/wiki/Cancelled_(South_Park)). It looks like they were right even about their own show. The numbers simply don't lie.

## Conclusion

In this article, you’ve learned that sentiment analysis scores words using a subjective dictionary. You've also seen how to use such information to get an overall fell of the show. We've put it all together to make an interactive plot with just a few lines of R code.

I have shown you that once you have an idea, nothing is impossible. Answering your own questions using R is easy. Be curious and do what you like! Even though the overall rating of the show keeps decreasing, I will not stop watching it. I still like it a lot! The next article in the series will focus on the main characters. You will learn how their individual sentiments evolve apart from other things. Stay tuned to see how they differ from each other!

Knowing R is a very valuable skill nowadays. You can start with the basics at [Vertabelo Academy](https://academy.vertabelo.com/). I will personally recommend learning the [Tidyverse](https://academy.vertabelo.com/course/tidyverse). I use it in every analysis and I can't really imagine a woRld without it!

If you already know R and want to explore the data on your own, check out my [GitHub repository](https://github.com/pdrhlik/vertabelo/tree/master/going-down-to-south-park-to-make-a-text-analysis-using-r). The page comes with instructions on how to do that. Good luck and enjoy your own analyses!
